# GT-Whisper: Enhanced Whisper Transcription

GT-Whisper is an enhanced Whisper transcription system with glossary support and fuzzy post-correction, specifically designed for domain-specific content like ISKCON spiritual teachings.

## Features

### ðŸŽ¯ Work-stream B Implementation

This implementation covers all four components of Work-stream B:

1. **B1: Abstract Decode Wrapper** - Context-aware prompting with glossary integration
2. **B2: Pluggable Glossary Policy** - Multiple strategies for glossary selection
3. **B3: Fast Fuzzy Post-Correction** - Rapid error correction using Levenshtein distance
4. **B4: CLI & Minimal REST** - Command-line interface and optional REST API

### ðŸš€ Key Capabilities

- **Context-Aware Prompting**: Uses last 32 words from previous transcript as context
- **Glossary Integration**: Supports static, chapter-based, and empty glossary strategies
- **Fuzzy Post-Correction**: Corrects transcription errors using rapidfuzz with â‰¤2 edit distance
- **Multiple Output Formats**: Text, JSON, and SRT subtitle formats
- **Performance Monitoring**: Detailed statistics and performance tracking
- **Flexible Configuration**: Configurable parameters for all components

## Installation

### Dependencies

```bash
pip install rapidfuzz>=3.0.0
```

All other dependencies should already be available in the ISKCON-Translate project.

### Verify Installation

```bash
python test_gt_whisper.py
```

## Usage

### Command Line Interface

Basic usage:
```bash
python -m src.transcription audio.mp3
```

With glossary strategy:
```bash
python -m src.transcription audio.mp3 --strategy static_common
```

With custom glossary file:
```bash
python -m src.transcription audio.mp3 --glossary-path example_glossary.txt
```

Full example with all options:
```bash
python -m src.transcription audio.mp3 \
  --model medium \
  --device cuda \
  --language en \
  --strategy static_common \
  --glossary-path example_glossary.txt \
  --format json \
  --output result.json \
  --verbose
```

### CLI Options

| Option | Description | Default |
|--------|-------------|---------|
| `--model, -m` | Whisper model size (tiny, base, small, medium, large) | small |
| `--device, -d` | Device (auto, cpu, cuda) | auto |
| `--language, -l` | Language code (e.g., en, hi, sa) | auto-detect |
| `--strategy, -s` | Glossary strategy (static_common, chapter_guess, empty) | static_common |
| `--glossary-path` | Path to static glossary file | None |
| `--glossary-dir` | Directory with chapter-specific glossaries | None |
| `--no-post-correction` | Disable fuzzy post-correction | False |
| `--context-window` | Number of context words to use | 32 |
| `--output, -o` | Output file path | stdout |
| `--format, -f` | Output format (text, json, srt) | text |
| `--verbose, -v` | Enable verbose logging | False |
| `--quiet, -q` | Suppress output except errors | False |

### Python API

```python
from src.transcription import GTWhisper

# Initialize GT-Whisper
gt_whisper = GTWhisper(
    model_size="small",
    device="auto",
    strategy="static_common",
    glossary_path="example_glossary.txt",
    enable_post_correction=True,
    context_window=32
)

# Transcribe a file
result = gt_whisper.transcribe_file(
    audio_path="audio.mp3",
    language="en",
    output_format="json"
)

print(f"Transcribed: {result['text']}")
print(f"Corrections made: {result['corrections_made']}")
print(f"Processing time: {result['processing_time']:.2f}s")
```

### Individual Components

#### Decode Wrapper
```python
from src.transcription import DecodeWrapper, init_whisper_model

model, device, compute_type = init_whisper_model("small", "auto")
wrapper = DecodeWrapper(model, context_window=32)

# Load glossary
wrapper.load_glossary("example_glossary.txt")

# Transcribe with context
result = wrapper.transcribe("audio.mp3", language="en")
print(f"Text: {result.text}")
print(f"Glossary matches: {result.glossary_matches}")
```

#### Glossary Policy
```python
from src.transcription import create_glossary_selector

# Create selector with static common strategy
selector = create_glossary_selector(
    strategy="static_common",
    glossary_path="example_glossary.txt"
)

# Get glossary for context
context = {'audio_path': 'audio.mp3', 'previous_text': ''}
result = selector.get_glossary(context)
print(f"Strategy: {result.strategy_used}")
print(f"Glossary: {result.glossary_text[:100]}...")
```

#### Post-Processor
```python
from src.transcription import FuzzyPostProcessor

processor = FuzzyPostProcessor(max_distance=2, min_confidence=80.0)
processor.load_terms_from_file("example_glossary.txt")

result = processor.correct_text("krisna teaches arjun about dharma")
print(f"Original: {result.original_text}")
print(f"Corrected: {result.corrected_text}")
print(f"Corrections: {len(result.corrections_made)}")
```

## Glossary Strategies

### Static Common (`static_common`)
- Uses a predefined set of common ISKCON/spiritual terms
- Always applies the same glossary regardless of context
- High confidence (1.0) since terms are always relevant
- Best for general spiritual content

### Chapter Guess (`chapter_guess`)
- Attempts to guess chapter/topic from filename or context
- Looks for patterns like "chapter1", "bg2", "sb3" in filenames
- Falls back to common terms if no chapter is identified
- Best for structured content with clear chapter organization

### Empty (`empty`)
- Returns no glossary terms (control strategy)
- Used for testing the impact of glossary enhancement
- Always confident (1.0) in returning nothing
- Best for baseline comparisons

## Glossary File Format

The glossary file supports two formats:

1. **Simple terms** (one per line):
```
Krishna
Arjuna
Bhagavad Gita
```

2. **Correction mappings** (incorrect -> correct):
```
krisna -> Krishna
arjun -> Arjuna
bhagwad -> Bhagavad
```

Comments start with `#` and are ignored.

## Post-Correction Features

The fuzzy post-processor provides:

- **Levenshtein Distance**: Maximum edit distance of 2 for matches
- **Confidence Threshold**: Minimum 80% similarity for corrections
- **Caching**: Frequently corrected terms are cached for speed
- **Performance Tracking**: Statistics on correction rates and timing
- **Configurable Parameters**: Adjustable distance and confidence thresholds

### Default Corrections

The system includes built-in corrections for common ISKCON terms:

| Incorrect | Correct |
|-----------|---------|
| krisna, krsna | Krishna |
| arjun | Arjuna |
| bhagwad | Bhagavad |
| geeta | Gita |
| iskon | ISKCON |
| prabhupad | Prabhupada |
| And many more... | |

## Performance

### Benchmarks

Based on testing with the "small" Whisper model:

- **Context Building**: ~0.1ms per prompt
- **Glossary Loading**: ~1-5ms depending on size
- **Post-Correction**: ~2-10ms per sentence
- **Overall Overhead**: ~5-15% of base transcription time

### Memory Usage

- **Glossary Storage**: ~1-10KB per strategy
- **Correction Cache**: Grows with usage, typically <1MB
- **Context Buffer**: ~1KB for 32-word window

## Integration with ISKCON-Translate

GT-Whisper is designed to integrate seamlessly with the existing transcription engine:

```python
# In your existing code, replace:
# from src.transcription import TranscriptionEngine

# With:
from src.transcription import GTWhisper

# Initialize with enhanced features
engine = GTWhisper(
    model_size="small",
    strategy="static_common",
    enable_post_correction=True
)
```

## REST API (Optional)

If Flask is installed, GT-Whisper can run as a REST service:

```python
from src.transcription.gt_whisper import create_rest_api

app = create_rest_api()
app.run(host='0.0.0.0', port=5000)
```

### Endpoints

- `POST /transcribe` - Upload audio file for transcription
- `GET /health` - Health check
- `GET /stats` - Performance statistics

## Testing

Run the test suite:
```bash
python test_gt_whisper.py
```

Test CLI help:
```bash
python -m src.transcription --help
```

## Configuration Examples

### High Accuracy Setup
```bash
python -m src.transcription audio.mp3 \
  --model large \
  --device cuda \
  --strategy static_common \
  --glossary-path example_glossary.txt \
  --context-window 64
```

### Fast Processing Setup
```bash
python -m src.transcription audio.mp3 \
  --model tiny \
  --device cpu \
  --strategy empty \
  --no-post-correction \
  --context-window 16
```

### Chapter-Specific Setup
```bash
python -m src.transcription bg_chapter_2.mp3 \
  --strategy chapter_guess \
  --glossary-dir ./chapter_glossaries/
```

## Troubleshooting

### Common Issues

1. **ModuleNotFoundError: rapidfuzz**
   ```bash
   pip install rapidfuzz>=3.0.0
   ```

2. **CUDA not available**
   - Use `--device cpu` for CPU-only processing
   - Install CUDA drivers for GPU acceleration

3. **Glossary file not found**
   - Check file path and permissions
   - Use absolute paths if relative paths fail

4. **Poor correction quality**
   - Adjust `--context-window` size
   - Customize glossary file for your domain
   - Tune post-correction parameters in code

### Debug Mode

Enable verbose logging for debugging:
```bash
python -m src.transcription audio.mp3 --verbose
```

## Contributing

To extend GT-Whisper:

1. **Add new glossary strategies**: Inherit from `BaseGlossaryPolicy`
2. **Customize post-correction**: Extend `FuzzyPostProcessor`
3. **Add output formats**: Modify `GTWhisper._save_output()`
4. **Enhance context building**: Extend `DecodeWrapper.build_initial_prompt()`

## License

Same as ISKCON-Translate project.

## Acknowledgments

- Built on top of faster-whisper
- Uses rapidfuzz for efficient fuzzy matching
- Designed for ISKCON spiritual content transcription 